{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../Data/df_train.csv')\n",
    "df_val = pd.read_csv('../Data/df_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dropout, Conv2DTranspose, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_model(input_shape, num_classes):\n",
    "    inputs = Input(input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same')(pool1)\n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same')(pool2)\n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = Conv2D(512, 3, activation='relu', padding='same')(pool3)\n",
    "    conv4 = Conv2D(512, 3, activation='relu', padding='same')(conv4)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "    conv5 = Conv2D(1024, 3, activation='relu', padding='same')(pool4)\n",
    "    conv5 = Conv2D(1024, 3, activation='relu', padding='same')(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "    # Decoder\n",
    "    up6 = Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(drop5)\n",
    "    up6 = concatenate([up6, drop4], axis=3)\n",
    "    conv6 = Conv2D(512, 3, activation='relu', padding='same')(up6)\n",
    "    conv6 = Conv2D(512, 3, activation='relu', padding='same')(conv6)\n",
    "\n",
    "    up7 = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv6)\n",
    "    up7 = concatenate([up7, conv3], axis=3)\n",
    "    conv7 = Conv2D(256, 3, activation='relu', padding='same')(up7)\n",
    "    conv7 = Conv2D(256, 3, activation='relu', padding='same')(conv7)\n",
    "\n",
    "    up8 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv7)\n",
    "    up8 = concatenate([up8, conv2], axis=3)\n",
    "    conv8 = Conv2D(128, 3, activation='relu', padding='same')(up8)\n",
    "    conv8 = Conv2D(128, 3, activation='relu', padding='same')(conv8)\n",
    "\n",
    "    up9 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv8)\n",
    "    up9 = concatenate([up9, conv1], axis=3)\n",
    "    conv9 = Conv2D(64, 3, activation='relu', padding='same')(up9)\n",
    "    conv9 = Conv2D(64, 3, activation='relu', padding='same')(conv9)\n",
    "\n",
    "    outputs = Conv2D(num_classes, (1, 1), activation='softmax')(conv9)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "\n",
    "    return model\n",
    "\n",
    "input_shape = (256, 256, 3)  # Taille d'entrée de l'image\n",
    "num_classes = 8\n",
    "model = unet_model(input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "    A.HorizontalFlip(p=1.0)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape2 = (256, 256, 3)\n",
    "\n",
    "def load_data(df):\n",
    "    images_tab = []\n",
    "    masks_tab = []\n",
    "    for i in range(len(df)):\n",
    "        image_path = df['Image_Path'][i]\n",
    "        mask_path = df['Target_Path'][i]\n",
    "        image = img_to_array(Image.open(image_path).convert('RGB').resize(input_shape[:2]))\n",
    "        image = image.astype(np.uint8)\n",
    "        images_tab.append(image)\n",
    "\n",
    "        mask = img_to_array(Image.open(mask_path).resize(input_shape2[:2], Image.NEAREST))\n",
    "        mask = mask.astype(np.uint8)\n",
    "        for x in range(mask.shape[0]):\n",
    "            for y in range(mask.shape[1]):\n",
    "                if (mask[x, y] == [250, 170, 30]).all():\n",
    "                    mask[x, y] = 0\n",
    "                elif (mask[x, y] == [0, 0, 142]).all():\n",
    "                    mask[x, y] = 1\n",
    "                elif (mask[x, y] == [102, 102, 156]).all():\n",
    "                    mask[x, y] = 2\n",
    "                elif (mask[x, y] == [220, 20, 60]).all():\n",
    "                    mask[x, y] = 3\n",
    "                elif (mask[x, y] == [153, 153, 153]).all():\n",
    "                    mask[x, y] = 4\n",
    "                elif (mask[x, y] == [244, 35, 232]).all():\n",
    "                    mask[x, y] = 5\n",
    "                elif (mask[x, y] == [70, 70, 70]).all():\n",
    "                    mask[x, y] = 6\n",
    "                elif (mask[x, y] == [70, 130, 180]).all():\n",
    "                    mask[x, y] = 7\n",
    "        mask = mask[:, :, 0:1]\n",
    "        masks_tab.append(mask)\n",
    "\n",
    "        transformed = transform(image=image, mask=mask)\n",
    "        transformed_img = transformed['image']\n",
    "        images_tab.append(transformed_img)\n",
    "\n",
    "        transformed_img = transformed['mask']\n",
    "        for x in range(transformed_img.shape[0]):\n",
    "            for y in range(transformed_img.shape[1]):\n",
    "                if (transformed_img[x, y] == [250, 170, 30]).all():\n",
    "                    transformed_img[x, y] = 0\n",
    "                elif (transformed_img[x, y] == [0, 0, 142]).all():\n",
    "                    transformed_img[x, y] = 1\n",
    "                elif (transformed_img[x, y] == [102, 102, 156]).all():\n",
    "                    transformed_img[x, y] = 2\n",
    "                elif (transformed_img[x, y] == [220, 20, 60]).all():\n",
    "                    transformed_img[x, y] = 3\n",
    "                elif (transformed_img[x, y] == [153, 153, 153]).all():\n",
    "                    transformed_img[x, y] = 4\n",
    "                elif (transformed_img[x, y] == [244, 35, 232]).all():\n",
    "                    transformed_img[x, y] = 5\n",
    "                elif (transformed_img[x, y] == [70, 70, 70]).all():\n",
    "                    transformed_img[x, y] = 6\n",
    "                elif (transformed_img[x, y] == [70, 130, 180]).all():\n",
    "                    transformed_img[x, y] = 7\n",
    "        transformed_img = transformed_img[:, :, 0:1]\n",
    "        masks_tab.append(transformed_img)\n",
    "\n",
    "    images_tab = np.array(images_tab)\n",
    "    masks_tab = np.array(masks_tab)\n",
    "    return images_tab, masks_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total d'images chargées: 5950\n",
      "Nombre total de masks chargées: 5950\n",
      "Nombre total d'images de validation chargées: 1000\n",
      "Nombre total de masks de validation chargées: 1000\n",
      "Shape images : (5950, 256, 256, 3)\n",
      "Shape masks : (5950, 256, 256, 1)\n",
      "Shape images_val : (1000, 256, 256, 3)\n",
      "Shape masks_val : (1000, 256, 256, 1)\n"
     ]
    }
   ],
   "source": [
    "images, masks = load_data(df_train)\n",
    "images_val, masks_val = load_data(df_val)\n",
    "print(\"Nombre total d'images chargées:\", len(images))\n",
    "print(\"Nombre total de masks chargées:\", len(masks))\n",
    "print(\"Nombre total d'images de validation chargées:\", len(images_val))\n",
    "print(\"Nombre total de masks de validation chargées:\", len(masks_val))\n",
    "print(\"Shape images :\", images.shape)\n",
    "print(\"Shape masks :\", masks.shape)\n",
    "print(\"Shape images_val :\", images_val.shape)\n",
    "print(\"Shape masks_val :\", masks_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = images.copy()\n",
    "X_val = images_val.copy()\n",
    "y_train = masks.copy()\n",
    "y_val = masks_val.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67s/step - accuracy: 0.4931 - loss: 2.1748  \n",
      "Epoch 1: val_loss improved from inf to 0.78218, saving model to model_X.keras\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12925s\u001b[0m 69s/step - accuracy: 0.4938 - loss: 2.1702 - val_accuracy: 0.7580 - val_loss: 0.7822\n",
      "Epoch 2/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66s/step - accuracy: 0.8020 - loss: 0.6440  \n",
      "Epoch 2: val_loss improved from 0.78218 to 0.62846, saving model to model_X.keras\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12859s\u001b[0m 69s/step - accuracy: 0.8021 - loss: 0.6438 - val_accuracy: 0.8080 - val_loss: 0.6285\n",
      "Epoch 3/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67s/step - accuracy: 0.8410 - loss: 0.5216  \n",
      "Epoch 3: val_loss improved from 0.62846 to 0.56648, saving model to model_X.keras\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12920s\u001b[0m 69s/step - accuracy: 0.8410 - loss: 0.5216 - val_accuracy: 0.8315 - val_loss: 0.5665\n",
      "Epoch 4/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66s/step - accuracy: 0.8627 - loss: 0.4535  \n",
      "Epoch 4: val_loss improved from 0.56648 to 0.48519, saving model to model_X.keras\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12835s\u001b[0m 69s/step - accuracy: 0.8627 - loss: 0.4535 - val_accuracy: 0.8541 - val_loss: 0.4852\n",
      "Epoch 5/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69s/step - accuracy: 0.8734 - loss: 0.4176  \n",
      "Epoch 5: val_loss improved from 0.48519 to 0.48451, saving model to model_X.keras\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13433s\u001b[0m 72s/step - accuracy: 0.8734 - loss: 0.4175 - val_accuracy: 0.8553 - val_loss: 0.4845\n",
      "Epoch 6/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69s/step - accuracy: 0.8824 - loss: 0.3867  \n",
      "Epoch 6: val_loss improved from 0.48451 to 0.44992, saving model to model_X.keras\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13434s\u001b[0m 72s/step - accuracy: 0.8824 - loss: 0.3867 - val_accuracy: 0.8626 - val_loss: 0.4499\n",
      "Epoch 7/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69s/step - accuracy: 0.8868 - loss: 0.3675  \n",
      "Epoch 7: val_loss improved from 0.44992 to 0.42007, saving model to model_X.keras\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13343s\u001b[0m 72s/step - accuracy: 0.8868 - loss: 0.3675 - val_accuracy: 0.8720 - val_loss: 0.4201\n",
      "Epoch 8/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68s/step - accuracy: 0.8939 - loss: 0.3418  \n",
      "Epoch 8: val_loss improved from 0.42007 to 0.38842, saving model to model_X.keras\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13239s\u001b[0m 71s/step - accuracy: 0.8939 - loss: 0.3418 - val_accuracy: 0.8796 - val_loss: 0.3884\n",
      "Epoch 9/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68s/step - accuracy: 0.8970 - loss: 0.3320  \n",
      "Epoch 9: val_loss improved from 0.38842 to 0.38368, saving model to model_X.keras\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13089s\u001b[0m 70s/step - accuracy: 0.8970 - loss: 0.3320 - val_accuracy: 0.8825 - val_loss: 0.3837\n",
      "Epoch 10/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68s/step - accuracy: 0.8995 - loss: 0.3190  \n",
      "Epoch 10: val_loss improved from 0.38368 to 0.37601, saving model to model_X.keras\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13149s\u001b[0m 71s/step - accuracy: 0.8995 - loss: 0.3190 - val_accuracy: 0.8840 - val_loss: 0.3760\n",
      "Epoch 11/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73s/step - accuracy: 0.9028 - loss: 0.3070  \n",
      "Epoch 11: val_loss did not improve from 0.37601\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14147s\u001b[0m 76s/step - accuracy: 0.9028 - loss: 0.3069 - val_accuracy: 0.8816 - val_loss: 0.3772\n",
      "Epoch 12/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72s/step - accuracy: 0.9061 - loss: 0.2952  \n",
      "Epoch 12: val_loss did not improve from 0.37601\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13946s\u001b[0m 75s/step - accuracy: 0.9061 - loss: 0.2952 - val_accuracy: 0.8817 - val_loss: 0.4573\n",
      "Epoch 13/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71s/step - accuracy: 0.9056 - loss: 0.2958  \n",
      "Epoch 13: val_loss did not improve from 0.37601\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13780s\u001b[0m 74s/step - accuracy: 0.9056 - loss: 0.2957 - val_accuracy: 0.8869 - val_loss: 0.3769\n",
      "Epoch 13: early stopping\n",
      "Restoring model weights from the end of the best epoch: 10.\n"
     ]
    }
   ],
   "source": [
    "model = unet_model(input_shape, num_classes)\n",
    "model.compile(optimizer=Adam(), loss=SparseCategoricalCrossentropy(), metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('model_X.keras', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "                    validation_data=(X_val, y_val), \n",
    "                    batch_size=32, \n",
    "                    epochs=100, \n",
    "                    callbacks=[early_stopping, model_checkpoint], \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VisionTransport",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
